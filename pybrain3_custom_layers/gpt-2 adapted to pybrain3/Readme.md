# GPT-2 now uses numpy? What gives?

The version here uses numpy & Scipy instead of tensorflow. The choice was for simplified remodeling later so it had the proper layers, layout, and basic functionality.

``pybrain3`` allows for similar imitation but further reduces the potential for error. Beyond that, test as you'd like as this version was not written by a proprietary company.

As the model I am building, while it touches on similar approach, it requires the neurochemical and biochemical simulator bits and eeg/bci data for genuine AI to be modeled.
Token parsing is not good enough as token parsing, as gpt-2, 3, & 4, as presently built by openAI developers, is nothing more than a suffisticated form of the Eliza project, designed to defeat the purpose of the turing test with the intent on fooling the user with clever symbolic references. 

Due to openAI's, and many others who failed to pay attention to what Alan Turing said, Alan Turing's version of AI, AI will be built by open source developers like myself.

# why does that differential matter? Isn't attention all that is needed?

Sure, if one is developing it, but, that lacks emotional reasoning and argues that inferencing and symbolic references is all that's needed. One question: Where is it's brain in
 the process? No where, so it can't be called "sentient" or "artificial intelligence". By that notion, you might as well call your coffee table or smart fridge "AI". Or for that matter, your dresser. Is your dresser or coffee table sentient and harboring artificial intelligence? I'm pretty sure you'd agree that the last 2 questions are an indication of why assuming token parsing and attention is not all that is needed for AI.

# So the hype about "AI" and what's on the market currently isn't AI?

Correct. It's as useful as the Eliza projects in conversation and "reasoning" \*\*ahem\*\*. 

# How long before your model is an AI based on your pybrain3 code?

Depends, really. I am aiming for a AI development job. So, It will take significantly longer than initially anticipated. It also depends on the training it gets, data used, human interactive testing, etc. And when new layers get added it isn't simply transfering the data and recalibrating the old and new layer. It is more or less retraining the entire model.

I'd say it would take someone from this point on 3 months flat with a kubernetes cluster at their disposal. If they're using k3d/l3s on their laptop or cell phone, probably a bit longer. Even then, it has to go through the same tests a normal human would ( think of how a neurologist & psychologist break down problems ). It would take just as long to to train. For instance, it needs to be able to emulate a GABA, CB1, & CB2 junctors most if not all living beings with a brain would. In this case, I'm aiming for it to have enough
 to emulate/simulate til it isn't a simulated or emulated version and proves to have a 1:1 ratio to human nerve cells work. 

# Then what, have it map their neural patterns to genetic templates?

Yes, actually. It's one function of the brain. Our genetics in our heads change and why should the AI templates do any different in that respect? Sure, there are bound to be errors, but those errors will be easier to pinpoint as human error rather than assuming it to be natural phenomenon. If a person changes IQ ranges, the genes governing that reshape due to the alternative which would be death (in this case, it decreases rather than increases..."death"....).

However, this would also give more insight to how diseases, disorders, and the like. It would be more useful to know with a higher degree of precision on what is failing and where to look rather than guessing. 
