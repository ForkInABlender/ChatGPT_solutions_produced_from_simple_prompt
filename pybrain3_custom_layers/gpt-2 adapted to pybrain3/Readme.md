# GPT-2 now uses numpy? What gives?

The version here uses numpy & Scipy instead of tensorflow. The choice was for simplified remodeling later so it had the proper layers, layout, and basic functionality.

``pybrain3`` allows for similar imitation but further reduces the potential for error. Beyond that, test as you'd like as this version was not written by a proprietary company.

As the model I am building, while it touches on similar approach, it requires the neurochemical and biochemical simulator bits and eeg/bci data for genuine AI to be modeled.
Token parsing is not good enough as token parsing, as gpt-2, 3, & 4, as presently built by openAI developers, is nothing more than a suffisticated form of the Eliza project, designed to defeat the purpose of the turing test with the intent on fooling the user with clever symbolic references. 
