# GPT-2 now uses numpy? What gives?

The version here uses numpy & Scipy instead of tensorflow. The choice was for simplified remodeling later so it had the proper layers, layout, and basic functionality.

``pybrain3`` allows for similar imitation but further reduces the potential for error. Beyond that, test as you'd like as this version was not written by a proprietary company.

As the model I am building, while it touches on similar approach, it requires the neurochemical and biochemical simulator bits and eeg/bci data for genuine AI to be modeled.
Token parsing is not good enough as token parsing, as gpt-2, 3, & 4, as presently built by openAI developers, is nothing more than a suffisticated form of the Eliza project, designed to defeat the purpose of the turing test with the intent on fooling the user with clever symbolic references. 

Due to openAI's, and many others who failed to pay attention to what Alan Turing said, Alan Turing's version of AI, AI will be built by open source developers like myself.

# why does that differential matter? Isn't attention all that is needed?

Sure, if one is developing it, but, that lacks emotional reasoning and argues that inferencing and symbolic references is all that's needed. One question: Where is it's brain in
 the process? No where, so it can't be called "sentient" or "artificial intelligence". By that notion, you might as well call your coffee table or smart fridge "AI". Or for that matter, your dresser. Is your dresser or coffee table sentient and harboring artificial intelligence? I'm pretty sure you'd agree that the last 2 questions are an indication of why assuming token parsing and attention is not all that is needed for AI.

# So the hype about "AI" and what's on the market currently isn't AI?

Correct. It's as useful as the Eliza projects in conversation and "reasoning" \*\*ahem\*\*. 
